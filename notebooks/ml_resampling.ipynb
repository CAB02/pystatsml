{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left out samples validation\n",
    "\n",
    "The **training error** can be easily calculated by applying the statistical learning method to the observations used in its training. But because of overfitting, the training error rate can dramatically underestimate the error that would be obtained on new samples.\n",
    "\n",
    "\n",
    "The **test error** is the average error that results from a learning method to predict the response on a new samples that is, on samples that were not used in training the method. Given a data set, the use of a particular learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case.\n",
    "\n",
    "Thus the original dataset is generally splited in a training and a test (or validation) data sets. Large training set (80%) small test set (20%) might provide a poor estimation of the predictive performances. On the contrary, large test set and small training set might produce a poorly estimated learner. This is why, on situation where we cannot afford such split, it recommended to use cross-Validation scheme to estimate the predictive power of a learning algorithm.\n",
    "\n",
    "\n",
    "## Cross-Validation (CV)\n",
    "\n",
    "Cross-Validation scheme randomly divid the set of observations into $K$ groups, or **folds**, of approximately equal size. The first fold is treated as a validation set, and the method $f()$ is fitted on the remaining union of $K - 1$ folds: ($f(X_{-K}, y_{-K})$).\n",
    "\n",
    "The mean error measure (generally a loss function) is evaluated of the on the observations in the held-out fold. For each sample $i$ we consider the model estimated on the data set that did not contain it, noted $-K(i)$. This procedure is repeated $K$ times; each time, a different group of observations is treated as a test set.\n",
    "Then we compare the predicted value ($f(X_{-K(i)}) = \\hat{y_i})$ with true value $y_i$ using a Error function $L()$. Then the cross validation estimate of prediction error is\n",
    "\n",
    "$$\n",
    "CV(f) = \\frac{1}{N} \\sum_i^N L\\left(y_i, f(X_{-K(i)}) \\right).\n",
    "$$\n",
    "\n",
    "This validation scheme is known as the **K-Fold CV**. Typical choices of $K$ are 5 or 10, [Kohavi 1995]. The extreme case where $K = N$ is known as **leave-one-out cross-validation, LOO-CV**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV for regression\n",
    "\n",
    "Usually the error function $L()$ is the r-squared score. However other function could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2:0.99\n",
      "Test  r2:0.72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=100, \n",
    "                         n_informative=10, random_state=42)\n",
    "model = lm.Ridge(alpha=10)\n",
    "\n",
    "cv = KFold(len(y), n_folds=5, random_state=42)\n",
    "y_test_pred = np.zeros(len(y))\n",
    "y_train_pred = np.zeros(len(y))\n",
    "\n",
    "for train, test in cv:\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred[test] = model.predict(X_test)\n",
    "    y_train_pred[train] = model.predict(X_train)\n",
    "\n",
    "print(\"Train r2:%.2f\" % metrics.r2_score(y, y_train_pred))\n",
    "print(\"Test  r2:%.2f\" % metrics.r2_score(y, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides user-friendly function to perform CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  r2:0.73\n",
      "Test  r2:0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "# provide a cv\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=cv)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV for classification\n",
    "\n",
    "With classification problems it is essential to sample folds where each set contains approximately the same percentage of samples of each target class as the complete set. This is called **stratification**. In this case, we will use ``StratifiedKFold`` with is a variation of k-fold which returns stratified folds.\n",
    "\n",
    "Usually the error function $L()$ are, at least, the sensitivity and the specificity. However other function could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SPC:1.00; SEN:1.00\n",
      "Test  SPC:0.80; SEN:0.82\n",
      "Test  ACC:0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=100, \n",
    "                         n_informative=10, random_state=42)\n",
    "\n",
    "model = lm.LogisticRegression(C=1)\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=5)\n",
    "y_test_pred = np.zeros(len(y))\n",
    "y_train_pred = np.zeros(len(y))\n",
    "\n",
    "for train, test in cv:\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred[test] = model.predict(X_test)\n",
    "    y_train_pred[train] = model.predict(X_train)\n",
    "\n",
    "recall_test  = metrics.recall_score(y, y_test_pred, average=None)\n",
    "recall_train = metrics.recall_score(y, y_train_pred, average=None)\n",
    "acc_test = metrics.accuracy_score(y, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"Train SPC:%.2f; SEN:%.2f\" % tuple(recall_train))\n",
    "print(\"Test  SPC:%.2f; SEN:%.2f\" % tuple(recall_test))\n",
    "print(\"Test  ACC:%.2f\" % acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides user-friendly function to perform CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  ACC:0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "scores.mean()\n",
    "\n",
    "# provide CV and score\n",
    "def balanced_acc(estimator, X, y):\n",
    "    '''\n",
    "    Balanced acuracy scorer\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=cv, scoring=balanced_acc)\n",
    "print(\"Test  ACC:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with Scikit-learn user-friendly function we average the scores' average obtained on individual folds which may provide slightly different results that the overall average presented earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV for model selection: setting the hyper parameters\n",
    "\n",
    "It is important to note CV may be used for two separate goals:\n",
    "\n",
    "1. **Model assessment**: having chosen a final model, estimating its prediction error (generalization error) on new data.\n",
    "\n",
    "2. **Model selection**: estimating the performance of different models in order to choose the best one. One special case of model selection is the selection model's hyper parameters. Indeed remember that most of learning algorithm have a hyper parameters (typically the regularization parameter) that has to be set.\n",
    "\n",
    "Generally we must address the two problems simultaneously. The usual approach for both problems is to randomly divide the dataset into three parts: a training set, a validation set, and a test set.\n",
    "\n",
    "- The **training set** (train) is used to fit the models;\n",
    "\n",
    "- the **validation set** (val) is used to estimate prediction error for model selection or to determine the hyper parameters over a grid of possible values.\n",
    "\n",
    "- the **test set** (test) is used for assessment of the generalization error of the final chosen model.\n",
    "\n",
    "\n",
    "###Â Grid search procedure\n",
    "\n",
    "Model selection of the best hyper parameters over a grid of possible values\n",
    "\n",
    "For each possible values of hyper parameters $\\alpha_k$:\n",
    "\n",
    "1. Fit the learner on training set: $f(X_{train}, y_{train}, \\alpha_k)$\n",
    "\n",
    "2. Evaluate the model on the validation set and keep the parameter(s) that minimises the error measure\n",
    "\n",
    "    $\\alpha_* = \\arg \\min L(f(X_{train}), y_{val}, \\alpha_k)$\n",
    "\n",
    "3. Refit the learner on all training + validation data using the best hyper parameters: $f^* \\equiv f(X_{train \\cup val}, y_{train \\cup val}, \\alpha_*)$\n",
    "\n",
    "4. ** Model assessment ** of $f^*$ on the test set: $L(f^*(X_{test}), y_{test})$\n",
    "\n",
    "### Nested CV for model selection and assessment\n",
    "\n",
    "Most of time, we cannot afford such three-way split. Thus, again we will use CV, but in this case we need two nested CVs.\n",
    "\n",
    "One **outer CV loop, for model assessment**. This CV performs $K$ splits of the dataset into training plus validation ($X_{-K}, y_{-K}$) set and a test set $X_{K}, y_{K}$\n",
    "\n",
    "One **inner CV loop, for model selection**. For each run of the outer loop, the inner loop loop performs $L$ splits of dataset ($X_{-K}, y_{-K}$) into training set: ($X_{-K,-L}, y_{-K,-L}$) and a validation set: ($X_{-K,L}, y_{-K,L}$).\n",
    "\n",
    "### Implementation with scikit-learn\n",
    "\n",
    "Note that the inner CV loop combined with the learner form a new learner with an automatic model (parameter) selection procedure. This new learner can be easily constructed using Scikit-learn. The learned is wrapped inside a ``GridSearchCV`` class.\n",
    "\n",
    "Then the new learned can be pluged into the classical outer CV loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 2.63584694464\n",
      "Train r2:0.96\n",
      "{'l1_ratio': 0.9, 'alpha': 1.0}\n",
      "Train r2:1.00\n",
      "Test  r2:0.62\n",
      "Selected alphas: [{'l1_ratio': 0.9, 'alpha': 0.001}, {'l1_ratio': 0.9, 'alpha': 0.001}, {'l1_ratio': 0.9, 'alpha': 0.001}, {'l1_ratio': 0.9, 'alpha': 0.01}, {'l1_ratio': 0.9, 'alpha': 0.001}]\n",
      "Test  r2:0.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Dataset\n",
    "noise_sd = 10\n",
    "X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=noise_sd,\n",
    "                         n_informative=2, random_state=42, coef=True)\n",
    " \n",
    "# Use this to tune the noise parameter such that snr < 5\n",
    "print(\"SNR:\", np.std(np.dot(X, coef)) / noise_sd)\n",
    "\n",
    "# param grid over alpha & l1_ratio\n",
    "param_grid = {'alpha': 10. ** np.arange(-3, 3), 'l1_ratio':[.1, .5, .9]}\n",
    "\n",
    "\n",
    "# Warp \n",
    "model = GridSearchCV(lm.ElasticNet(max_iter=10000), param_grid, cv=5)\n",
    "    \n",
    "# 1) Biased usage: fit on all data, ommit outer CV loop                 \n",
    "model.fit(X, y)\n",
    "print(\"Train r2:%.2f\" % metrics.r2_score(y, model.predict(X)))\n",
    "print(model.best_params_)\n",
    "\n",
    "# 2) User made outer CV, usefull to extract specific imformation \n",
    "cv = KFold(len(y), n_folds=5, random_state=42)\n",
    "y_test_pred = np.zeros(len(y))\n",
    "y_train_pred = np.zeros(len(y))\n",
    "alphas = list()\n",
    "\n",
    "for train, test in cv:\n",
    "    X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred[test] = model.predict(X_test)\n",
    "    y_train_pred[train] = model.predict(X_train)\n",
    "    alphas.append(model.best_params_)\n",
    "\n",
    "print(\"Train r2:%.2f\" % metrics.r2_score(y, y_train_pred))\n",
    "print(\"Test  r2:%.2f\" % metrics.r2_score(y, y_test_pred))\n",
    "print(\"Selected alphas:\", alphas)\n",
    "\n",
    "# 3.) user-friendly sklearn for outer CV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=cv)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression models with built-in cross-validation\n",
    "\n",
    "Sklearn wil automaticaly select a grib of parameters, most of time use the defaults values.\n",
    "\n",
    "``n_jobs`` is the number of CPUs to use during the cross validation. If -1, use all the CPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Ridge (L2 penalty) ==\n",
      "Test  r2:0.23\n",
      "== Lasso (L1 penalty) ==\n",
      "Test  r2:0.74\n",
      "== ElasticNet (L1 penalty) ==\n",
      "Test  r2:0.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Dataset\n",
    "X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=10,\n",
    "                         n_informative=2, random_state=42, coef=True)\n",
    " \n",
    "\n",
    "print(\"== Ridge (L2 penalty) ==\")\n",
    "model = lm.RidgeCV()\n",
    "# Let sklearn select a list of alphas with default LOO-CV\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"== Lasso (L1 penalty) ==\")\n",
    "model = lm.LassoCV(n_jobs=-1)\n",
    "# Let sklearn select a list of alphas with default 3CV\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"== ElasticNet (L1 penalty) ==\")\n",
    "model = lm.ElasticNetCV(l1_ratio=[.1, .5, .9], n_jobs=-1)\n",
    "# Let sklearn select a list of alphas with default 3CV\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models with built-in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Logistic Ridge (L2 penalty) ==\n",
      "Test  ACC:0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=100, \n",
    "                         n_informative=10, random_state=42)\n",
    "\n",
    "# provide CV and score\n",
    "def balanced_acc(estimator, X, y):\n",
    "    '''\n",
    "    Balanced acuracy scorer\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n",
    "\n",
    "print(\"== Logistic Ridge (L2 penalty) ==\")\n",
    "model = lm.LogisticRegressionCV(class_weight='balanced', scoring=balanced_acc, n_jobs=-1)\n",
    "# Let sklearn select a list of alphas with default LOO-CV\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  ACC:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Permutations\n",
    "\n",
    "A permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the the observations.\n",
    "\n",
    "Permutation tests are highly attractive because they make no assumptions other than that the observations are independent and identically distributed under the null hypothesis.\n",
    "\n",
    "1. Compute a obseverved statistic $t_{obs}$ on the data. \n",
    "2. Use randomization to compute the distribution of $t$ under the null hypothesis: Perform $N$ random permutation of the data. For each sample of permuted data, $i$ the data compute the statistic $t_i$. This procedure provides the distribution of $t$ under the null hypothesis $H_0$: $P(t \\vert H_0)$\n",
    "3. Compute the p-value = $P(t>t_{obs} | H_0) \\left\\vert\\{t_i > t_{obs}\\}\\right\\vert$, where $t_i$'s include $t_{obs}$.\n",
    "\n",
    "### Example with a correlation\n",
    "\n",
    "The statistic is the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation two tailed p-value=0.06959. Pearson test p-value=0.07355\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAF9CAYAAACtYqpnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx3Hyyv8DsPFPw+fJnkYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10ec4b1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(loc=10, scale=1, size=100)\n",
    "y = x + np.random.normal(loc=-3, scale=3, size=100) # snr = 1/2\n",
    "\n",
    "# Permutation: simulate the null hypothesis\n",
    "nperm = 10000\n",
    "perms = np.zeros(nperm + 1)\n",
    "\n",
    "perms[0] = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "for i in range(1, nperm):\n",
    "    perms[i] = np.corrcoef(np.random.permutation(x), y)[0, 1]\n",
    "\n",
    "# Plot\n",
    "#Â Re-weight to obtain distribution\n",
    "weights = np.ones(perms.shape[0]) / perms.shape[0]\n",
    "plt.hist([perms[perms >= perms[0]], perms], histtype='stepfilled', \n",
    "         bins=100, label=[\"t>t obs (p-value)\", \"t<t obs\"], \n",
    "         weights=[weights[perms >= perms[0]], weights])\n",
    "\n",
    "plt.xlabel(\"Statistic distribution under null hypothesis\")\n",
    "plt.axvline(x=perms[0], color='blue', linewidth=1, label=\"observed statistic\")\n",
    "_ = plt.legend(loc=\"upper left\")\n",
    "\n",
    "# One-tailed empirical p-value\n",
    "pval_perm = np.sum(perms >= perms[0]) / perms.shape[0]\n",
    "\n",
    "# Compare with Pearson's correlation test\n",
    "_, pval_test = stats.pearsonr(x, y)\n",
    "\n",
    "print(\"Permutation two tailed p-value=%.5f. Pearson test p-value=%.5f\" % (2*pval_perm, pval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Given the logistic regression presented above and its validation given a 5 folds CV.\n",
    "\n",
    "1. Compute the p-value associated with the prediction accuracy using a permutation test.\n",
    "\n",
    "2. Compute the p-value associated with the prediction accuracy using a parametric test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a random sampling with replacement strategy which provides an non-parametric method to assess the variability of performances scores such standard errors or confidence intervals. \n",
    "\n",
    "A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. \n",
    "\n",
    "1. Perform $B$ sampling, with replacement, of the dataset.\n",
    "2. For each sample $i$ fit the model and compute the scores.\n",
    "3. Assess standard errors and confidence intervals of scores using the scores obtained on the $B$ resampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients on all data:\n",
      "[ 0.98143428  0.84248041  0.12029217  0.09319979  0.08717254]\n",
      "r-squared: Mean=0.57, SE=0.09, CI=(0.39 0.70)\n",
      "Coefficients distribution\n",
      "                0           1           2           3           4\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
      "mean     0.975189    0.831922    0.116888    0.099109    0.085516\n",
      "std      0.106367    0.096548    0.108676    0.090312    0.091446\n",
      "min      0.745082    0.593736   -0.112740   -0.126522   -0.141713\n",
      "1%       0.770362    0.640142   -0.088238   -0.094403   -0.113375\n",
      "5%       0.787463    0.657473   -0.045593   -0.046201   -0.090458\n",
      "10%      0.829129    0.706492   -0.037838   -0.020650   -0.044990\n",
      "50%      0.980603    0.835724    0.133070    0.093240    0.088968\n",
      "95%      1.127518    0.999604    0.278735    0.251137    0.221887\n",
      "99%      1.144834    1.036715    0.292784    0.291197    0.287006\n",
      "max      1.146670    1.077265    0.324374    0.298135    0.289569\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Regression dataset\n",
    "n_features = 5\n",
    "n_features_info = 2\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "beta = np.zeros(n_features)\n",
    "beta[:n_features_info] = 1\n",
    "Xbeta = np.dot(X, beta)\n",
    "eps = np.random.randn(n_samples)\n",
    "y = Xbeta + eps\n",
    "\n",
    "# Fit model on all data (!! risk of overfit)\n",
    "model = lm.RidgeCV()\n",
    "model.fit(X, y)\n",
    "print(\"Coefficients on all data:\")\n",
    "print(model.coef_)\n",
    "\n",
    "# Bootstrap loop\n",
    "nboot = 100  # !! Should be at least 1000\n",
    "scores_names = [\"r2\"]\n",
    "scores_boot = np.zeros((nboot, len(scores_names)))\n",
    "coefs_boot = np.zeros((nboot, X.shape[1]))\n",
    "\n",
    "orig_all = np.arange(X.shape[0])\n",
    "for boot_i in range(nboot):\n",
    "    boot_tr = np.random.choice(orig_all, size=len(orig_all), replace=True)\n",
    "    boot_te = np.setdiff1d(orig_all, boot_tr, assume_unique=False)\n",
    "    Xtr, ytr = X[boot_tr, :], y[boot_tr]\n",
    "    Xte, yte = X[boot_te, :], y[boot_te]\n",
    "    model.fit(Xtr, ytr)\n",
    "    y_pred = model.predict(Xte).ravel()\n",
    "    scores_boot[boot_i, :] = metrics.r2_score(yte, y_pred)\n",
    "    coefs_boot[boot_i, :] = model.coef_\n",
    "\n",
    "# Compute Mean, SE, CI\n",
    "scores_boot = pd.DataFrame(scores_boot, columns=scores_names)\n",
    "scores_stat = scores_boot.describe(percentiles=[.99, .95, .5, .1, .05, 0.01])\n",
    "\n",
    "print(\"r-squared: Mean=%.2f, SE=%.2f, CI=(%.2f %.2f)\" %\\\n",
    "      tuple(scores_stat.ix[[\"mean\", \"std\", \"5%\", \"95%\"], \"r2\"]))\n",
    "\n",
    "coefs_boot = pd.DataFrame(coefs_boot)\n",
    "coefs_stat = coefs_boot.describe(percentiles=[.99, .95, .5, .1, .05, 0.01])\n",
    "print(\"Coefficients distribution\")\n",
    "print(coefs_stat)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
